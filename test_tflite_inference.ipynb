{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Test TFLite Model with Your Own Images\n",
    "\n",
    "This notebook lets you test the converted TFLite model with your own nail images.\n",
    "\n",
    "## Prerequisites:\n",
    "1. Run `convert_to_tflite.ipynb` first to generate the TFLite model\n",
    "2. Have the `.tflite` model file ready\n",
    "3. Prepare your test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install & Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow pillow numpy opencv-python matplotlib supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your TFLite model\n",
    "TFLITE_MODEL_PATH = \"./tflite_models/rfdetr_nails_float16.tflite\"\n",
    "\n",
    "# Check if model exists\n",
    "if not os.path.exists(TFLITE_MODEL_PATH):\n",
    "    print(f\"‚ùå Model not found at: {TFLITE_MODEL_PATH}\")\n",
    "    print(\"\\nPlease run convert_to_tflite.ipynb first to generate the model.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model found: {TFLITE_MODEL_PATH}\")\n",
    "    model_size = os.path.getsize(TFLITE_MODEL_PATH) / (1024 * 1024)\n",
    "    print(f\"üìä Model size: {model_size:.2f} MB\")\n",
    "\n",
    "# Load TFLite model\n",
    "print(\"\\nüì¶ Loading TFLite model...\")\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"\\nInput shape: {input_details[0]['shape']}\")\n",
    "print(f\"Output shape: {output_details[0]['shape']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Preprocess image for TFLite model\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        target_size: Target size (width, height)\n",
    "    \n",
    "    Returns:\n",
    "        preprocessed_image: Numpy array ready for inference\n",
    "        original_image: Original PIL Image\n",
    "        original_size: Original image size (width, height)\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "    original_size = original_image.size\n",
    "    \n",
    "    # Resize\n",
    "    resized_image = original_image.resize(target_size, Image.BILINEAR)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    img_array = np.array(resized_image).astype(np.float32)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    # Transpose to CHW format (channels, height, width)\n",
    "    img_array = np.transpose(img_array, (2, 0, 1))\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    return img_array, original_image, original_size\n",
    "\n",
    "print(\"‚úÖ Preprocessing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(image_path, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Run TFLite inference on image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        threshold: Confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        detections: Dictionary with detection results\n",
    "        inference_time: Inference time in ms\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    print(f\"\\nüì∏ Processing: {image_path}\")\n",
    "    input_data, original_image, original_size = preprocess_image(image_path)\n",
    "    \n",
    "    # Run inference\n",
    "    print(\"‚è±Ô∏è Running inference...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = (end_time - start_time) * 1000  # Convert to ms\n",
    "    \n",
    "    # Get output\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    print(f\"‚úÖ Inference completed in {inference_time:.2f} ms\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    \n",
    "    # TODO: Post-process output to extract detections\n",
    "    # This depends on your RF-DETR model's output format\n",
    "    # For now, return raw output\n",
    "    \n",
    "    return {\n",
    "        'output': output,\n",
    "        'original_image': original_image,\n",
    "        'original_size': original_size,\n",
    "        'inference_time': inference_time\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Inference function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test with Your Image\n",
    "\n",
    "### üìù Instructions:\n",
    "1. Put your test image in a known location\n",
    "2. Update the `IMAGE_PATH` variable below\n",
    "3. Run the cell to see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è UPDATE THIS PATH TO YOUR IMAGE\n",
    "IMAGE_PATH = \"/home/usama-naveed/nail_AR-rfdeter/usama_nails1.jpeg\"\n",
    "\n",
    "# Check if image exists\n",
    "if not os.path.exists(IMAGE_PATH):\n",
    "    print(f\"‚ùå Image not found: {IMAGE_PATH}\")\n",
    "    print(\"\\nPlease update IMAGE_PATH to point to your test image.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Image found: {IMAGE_PATH}\")\n",
    "    \n",
    "    # Run inference\n",
    "    result = run_inference(IMAGE_PATH, threshold=0.5)\n",
    "    \n",
    "    # Display original image\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(result['original_image'])\n",
    "    plt.title(f\"Original Image\\n{result['original_size'][0]}√ó{result['original_size'][1]}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display output visualization (if applicable)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(result['original_image'])\n",
    "    plt.title(f\"Inference Result\\nTime: {result['inference_time']:.2f} ms\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"  Image size: {result['original_size']}\")\n",
    "    print(f\"  Inference time: {result['inference_time']:.2f} ms\")\n",
    "    print(f\"  FPS: {1000/result['inference_time']:.1f}\")\n",
    "    print(f\"  Output shape: {result['output'].shape}\")\n",
    "    print(f\"  Output dtype: {result['output'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare with PyTorch Model (Optional)\n",
    "\n",
    "Run inference with both models and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PyTorch model for comparison\n",
    "try:\n",
    "    from rfdetr import RFDETRSegPreview\n",
    "    import torch\n",
    "    \n",
    "    print(\"üì¶ Loading PyTorch model...\")\n",
    "    pytorch_model = RFDETRSegPreview(\n",
    "        pretrain_weights=\"/home/usama-naveed/nail_AR-rfdeter/output/checkpoint_best_total.pth\"\n",
    "    )\n",
    "    pytorch_model.optimize_for_inference()\n",
    "    pytorch_model.eval()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        pytorch_model = pytorch_model.cuda()\n",
    "        device = \"CUDA\"\n",
    "    else:\n",
    "        device = \"CPU\"\n",
    "    \n",
    "    print(f\"‚úÖ PyTorch model loaded (device: {device})\")\n",
    "    \n",
    "    # Run PyTorch inference\n",
    "    print(\"\\n‚è±Ô∏è Running PyTorch inference...\")\n",
    "    image = Image.open(IMAGE_PATH).convert('RGB')\n",
    "    \n",
    "    start = time.time()\n",
    "    pytorch_result = pytorch_model.predict(image, threshold=0.5)\n",
    "    pytorch_time = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"‚úÖ PyTorch inference: {pytorch_time:.2f} ms\")\n",
    "    \n",
    "    # Compare\n",
    "    print(f\"\\nüìä Performance Comparison:\")\n",
    "    print(f\"  TFLite (Float16): {result['inference_time']:.2f} ms\")\n",
    "    print(f\"  PyTorch (Float32): {pytorch_time:.2f} ms\")\n",
    "    print(f\"  Speedup: {pytorch_time/result['inference_time']:.2f}x faster with TFLite\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load PyTorch model: {e}\")\n",
    "    print(\"Skipping comparison...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Batch Testing (Multiple Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple images\n",
    "import glob\n",
    "\n",
    "# ‚ö†Ô∏è UPDATE THIS PATH TO YOUR IMAGE FOLDER\n",
    "IMAGE_FOLDER = \"/home/usama-naveed/nail_AR-rfdeter/\"\n",
    "IMAGE_PATTERN = \"*.jpeg\"  # Change to *.jpg, *.png, etc.\n",
    "\n",
    "# Find all images\n",
    "image_files = glob.glob(os.path.join(IMAGE_FOLDER, IMAGE_PATTERN))\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    print(f\"‚ùå No images found matching: {os.path.join(IMAGE_FOLDER, IMAGE_PATTERN)}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {len(image_files)} images\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for img_path in image_files[:5]:  # Test first 5 images\n",
    "        print(f\"Processing: {os.path.basename(img_path)}\")\n",
    "        try:\n",
    "            result = run_inference(img_path, threshold=0.5)\n",
    "            results.append({\n",
    "                'path': img_path,\n",
    "                'time': result['inference_time'],\n",
    "                'size': result['original_size']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error: {e}\")\n",
    "    \n",
    "    # Show summary\n",
    "    if results:\n",
    "        times = [r['time'] for r in results]\n",
    "        print(f\"\\nüìä Batch Testing Summary:\")\n",
    "        print(f\"  Images processed: {len(results)}\")\n",
    "        print(f\"  Average time: {np.mean(times):.2f} ms\")\n",
    "        print(f\"  Min time: {np.min(times):.2f} ms\")\n",
    "        print(f\"  Max time: {np.max(times):.2f} ms\")\n",
    "        print(f\"  Average FPS: {1000/np.mean(times):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Output (If Applicable)\n",
    "\n",
    "This section will depend on your model's output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine raw output structure\n",
    "print(\"üîç Examining model output structure:\\n\")\n",
    "print(f\"Output shape: {result['output'].shape}\")\n",
    "print(f\"Output dtype: {result['output'].dtype}\")\n",
    "print(f\"Output range: [{result['output'].min():.4f}, {result['output'].max():.4f}]\")\n",
    "\n",
    "# If output is a single tensor, show its structure\n",
    "if len(result['output'].shape) == 4:\n",
    "    batch, channels, height, width = result['output'].shape\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"  Batch size: {batch}\")\n",
    "    print(f\"  Channels: {channels}\")\n",
    "    print(f\"  Height: {height}\")\n",
    "    print(f\"  Width: {width}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test results to file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "test_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model_path': TFLITE_MODEL_PATH,\n",
    "    'model_size_mb': os.path.getsize(TFLITE_MODEL_PATH) / (1024 * 1024),\n",
    "    'test_image': IMAGE_PATH,\n",
    "    'inference_time_ms': result['inference_time'],\n",
    "    'fps': 1000 / result['inference_time'],\n",
    "    'input_shape': input_details[0]['shape'].tolist(),\n",
    "    'output_shape': list(result['output'].shape)\n",
    "}\n",
    "\n",
    "results_file = \"tflite_test_results.json\"\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Test results saved to: {results_file}\")\n",
    "print(\"\\nüìÑ Results:\")\n",
    "print(json.dumps(test_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "You've successfully tested the TFLite model!\n",
    "\n",
    "### Next Steps:\n",
    "1. ‚úÖ Model is working and producing outputs\n",
    "2. üìä Note the inference time and FPS\n",
    "3. üîß Integrate into your backend (`model_rf_deter_tflite.py`)\n",
    "4. üöÄ Deploy to production\n",
    "\n",
    "### Key Metrics to Monitor:\n",
    "- **Inference Time:** Should be 2-3x faster than PyTorch\n",
    "- **Model Size:** Should be ~50% smaller\n",
    "- **Accuracy:** Should be similar (< 1% degradation)\n",
    "\n",
    "### Need Help?\n",
    "- Check `tflite_test_results.json` for detailed metrics\n",
    "- Compare with PyTorch model using Step 6\n",
    "- Adjust threshold in Step 5 if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
